{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hb-XtL_KOZ3"
      },
      "source": [
        "# **Pipeline de Prétraitement et Tokenisation pour la Détection de Sarcasme**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xinxCw8zWX3"
      },
      "source": [
        "**Importation des bibliothèques et préparation de l’environnement**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-3OIc8UzWkX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNpIHpeJzyXo"
      },
      "source": [
        "**CHARGEMENT DES DONNÉES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq1B9eojzyl8",
        "outputId": "6049bb69-4271-47e1-8588-2f0cc40f2914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre total d'exemples: 26709\n",
            "Distribution: is_sarcastic\n",
            "0    14985\n",
            "1    11724\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 1. CHARGEMENT DES DONNÉES\n",
        "# Charger le dataset de sarcasme\n",
        "# Format attendu: JSON avec 'headline' et 'is_sarcastic'\n",
        "with open('/content/Sarcasm_Headlines_Dataset.json', 'r') as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"Nombre total d'exemples: {len(df)}\")\n",
        "print(f\"Distribution: {df['is_sarcastic'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoOVLRJ_0R5G"
      },
      "source": [
        "**PRÉTRAITEMENT DES TEXTES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkolpQlJ0SIo",
        "outputId": "481444e6-214d-4564-8ebe-879142a75827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Exemple de prétraitement:\n",
            "Original: former versace store clerk sues over secret 'black code' for minority shoppers\n",
            "Propre: former versace store clerk sues over secret black code for minority shoppers\n"
          ]
        }
      ],
      "source": [
        "# 2. PRÉTRAITEMENT DES TEXTES\n",
        "def pretraiter_texte(texte):\n",
        "    \"\"\"\n",
        "    Nettoie le texte:\n",
        "    - Enlever les URLs\n",
        "    - Enlever les majuscules (convertir en minuscules)\n",
        "    - Garder seulement les caractères alphabétiques et espaces\n",
        "    \"\"\"\n",
        "    # Enlever les URLs\n",
        "    texte = re.sub(r'http\\S+|www\\S+|https\\S+', '', texte, flags=re.MULTILINE)\n",
        "    # Convertir en minuscules\n",
        "    texte = texte.lower()\n",
        "    # Enlever la ponctuation et garder seulement lettres et espaces\n",
        "    texte = re.sub(r'[^a-z\\s]', '', texte)\n",
        "    # Enlever les espaces multiples\n",
        "    texte = re.sub(r'\\s+', ' ', texte).strip()\n",
        "    return texte\n",
        "\n",
        "# Appliquer le prétraitement\n",
        "df['texte_propre'] = df['headline'].apply(pretraiter_texte)\n",
        "print(\"\\nExemple de prétraitement:\")\n",
        "print(f\"Original: {df['headline'].iloc[0]}\")\n",
        "print(f\"Propre: {df['texte_propre'].iloc[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2oul1Oi0T6S"
      },
      "source": [
        "**TOKENIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jIsluq80UId"
      },
      "outputs": [],
      "source": [
        "# 3. TOKENIZATION\n",
        "def tokeniser(texte):\n",
        "    \"\"\"Sépare le texte en mots (tokens)\"\"\"\n",
        "    return texte.split()\n",
        "\n",
        "# Tokeniser tous les textes\n",
        "df['tokens'] = df['texte_propre'].apply(tokeniser)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIEOnLrl06VQ"
      },
      "source": [
        "**CRÉATION DU VOCABULAIRE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKAZFKF206oW",
        "outputId": "0ac17cf1-95e9-4a8e-eb2e-9e96646acb68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Taille du vocabulaire: 5002\n",
            "Exemples de mots: ['<PAD>', '<UNK>', 'to', 'of', 'the', 'in', 'for', 'a', 'on', 'and']\n"
          ]
        }
      ],
      "source": [
        "# 4. CRÉATION DU VOCABULAIRE\n",
        "def creer_vocabulaire(liste_tokens, taille_max=5000):\n",
        "    \"\"\"\n",
        "    Crée un vocabulaire à partir des tokens\n",
        "    taille_max: nombre maximum de mots dans le vocabulaire\n",
        "    \"\"\"\n",
        "    # Compter tous les mots\n",
        "    tous_mots = []\n",
        "    for tokens in liste_tokens:\n",
        "        tous_mots.extend(tokens)\n",
        "\n",
        "    # Obtenir les mots les plus fréquents\n",
        "    compteur = Counter(tous_mots)\n",
        "    mots_frequents = compteur.most_common(taille_max)\n",
        "\n",
        "    # Créer le vocabulaire (mot -> index)\n",
        "    vocabulaire = {'<PAD>': 0, '<UNK>': 1}  # Tokens spéciaux\n",
        "    for idx, (mot, _) in enumerate(mots_frequents, start=2):\n",
        "        vocabulaire[mot] = idx\n",
        "\n",
        "    return vocabulaire\n",
        "\n",
        "vocabulaire = creer_vocabulaire(df['tokens'], taille_max=5000)\n",
        "print(f\"\\nTaille du vocabulaire: {len(vocabulaire)}\")\n",
        "print(f\"Exemples de mots: {list(vocabulaire.keys())[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMntmq_91Caf"
      },
      "source": [
        "**CONVERSION DES TOKENS EN INDICES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v4pau4a1Cq4"
      },
      "outputs": [],
      "source": [
        "# 5. CONVERSION DES TOKENS EN INDICES\n",
        "def tokens_vers_indices(tokens, vocab, max_len=20):\n",
        "    \"\"\"\n",
        "    Convertit une liste de tokens en indices\n",
        "    max_len: longueur maximale de la séquence\n",
        "    \"\"\"\n",
        "    indices = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "    # Padding ou troncature\n",
        "    if len(indices) < max_len:\n",
        "        indices += [vocab['<PAD>']] * (max_len - len(indices))\n",
        "    else:\n",
        "        indices = indices[:max_len]\n",
        "    return indices\n",
        "\n",
        "MAX_LENGTH = 20\n",
        "df['indices'] = df['tokens'].apply(lambda x: tokens_vers_indices(x, vocabulaire, MAX_LENGTH))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWrUX2lX1XIR"
      },
      "source": [
        "**PRÉPARATION DES DONNÉES POUR L'ENTRAÎNEMENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU4kwAUE1XXQ",
        "outputId": "4f3ac68c-6d15-4dce-b8cb-f62ea35dfa6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Formes des données:\n",
            "X_train: (21367, 20)\n",
            "y_train: (21367,)\n"
          ]
        }
      ],
      "source": [
        "# 6. PRÉPARATION DES DONNÉES POUR L'ENTRAÎNEMENT\n",
        "X = np.array(df['indices'].tolist())\n",
        "y = np.array(df['is_sarcastic'])\n",
        "\n",
        "# Division train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nFormes des données:\")\n",
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fTN6Jso1cs3"
      },
      "source": [
        "**COUCHE D'EMBEDDING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd6uzs5U1dqH"
      },
      "outputs": [],
      "source": [
        "# 7. COUCHE D'EMBEDDING\n",
        "class Embedding:\n",
        "    \"\"\"Couche d'embedding simple\"\"\"\n",
        "    def __init__(self, taille_vocab, dim_embedding):\n",
        "        self.taille_vocab = taille_vocab\n",
        "        self.dim_embedding = dim_embedding\n",
        "        # Initialisation aléatoire des embeddings\n",
        "        self.W = np.random.randn(taille_vocab, dim_embedding) * 0.01\n",
        "\n",
        "    def forward(self, indices):\n",
        "        \"\"\"\n",
        "        indices: (batch_size, max_length)\n",
        "        retourne: (batch_size, max_length, dim_embedding)\n",
        "        \"\"\"\n",
        "        return self.W[indices]\n",
        "\n",
        "    def backward(self, grad_output, indices):\n",
        "        \"\"\"Calcul du gradient pour les embeddings\"\"\"\n",
        "        grad_W = np.zeros_like(self.W)\n",
        "        np.add.at(grad_W, indices.flatten(), grad_output.reshape(-1, self.dim_embedding))\n",
        "        return grad_W\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjrXMaij1ht_"
      },
      "source": [
        "**FONCTION SOFTMAX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0cO66OT1h7p"
      },
      "outputs": [],
      "source": [
        "# 8. FONCTION SOFTMAX\n",
        "def softmax(x):\n",
        "    \"\"\"Calcule la softmax de manière stable numériquement\"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TCrZqwH1pj7"
      },
      "source": [
        "**CROSS-ENTROPY LOSS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTiXHK0R1px1"
      },
      "outputs": [],
      "source": [
        "# 9. CROSS-ENTROPY LOSS\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    y_pred: probabilités prédites (batch_size, 2)\n",
        "    y_true: labels vrais (batch_size,)\n",
        "    \"\"\"\n",
        "    batch_size = y_pred.shape[0]\n",
        "    # Éviter log(0)\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    # Loss pour chaque exemple\n",
        "    correct_probs = y_pred_clipped[range(batch_size), y_true]\n",
        "    loss = -np.mean(np.log(correct_probs))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UERUIxx1t5W"
      },
      "source": [
        "**MODÈLE SIMPLE DE CLASSIFICATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhQvfPem1uIV"
      },
      "outputs": [],
      "source": [
        "# 10. MODÈLE SIMPLE DE CLASSIFICATION\n",
        "class ClassifieurSarcasme:\n",
        "    \"\"\"Modèle simple: Embedding -> Moyenne -> Dense -> Softmax\"\"\"\n",
        "    def __init__(self, taille_vocab, dim_embedding, num_classes=2):\n",
        "        self.embedding = Embedding(taille_vocab, dim_embedding)\n",
        "        # Couche dense pour la classification\n",
        "        self.W = np.random.randn(dim_embedding, num_classes) * 0.01\n",
        "        self.b = np.zeros(num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        X: (batch_size, max_length)\n",
        "        \"\"\"\n",
        "        # Embedding\n",
        "        embedded = self.embedding.forward(X)  # (batch_size, max_length, dim_embedding)\n",
        "\n",
        "        # Moyenne sur la longueur de séquence\n",
        "        self.embedded_mean = np.mean(embedded, axis=1)  # (batch_size, dim_embedding)\n",
        "\n",
        "        # Couche dense\n",
        "        logits = np.dot(self.embedded_mean, self.W) + self.b  # (batch_size, num_classes)\n",
        "\n",
        "        # Softmax\n",
        "        probs = softmax(logits)\n",
        "\n",
        "        return probs\n",
        "\n",
        "    def backward(self, X, y_true, y_pred, learning_rate):\n",
        "        \"\"\"Rétropropagation et mise à jour des poids\"\"\"\n",
        "        batch_size = X.shape[0]\n",
        "\n",
        "        # Gradient de la softmax + cross-entropy\n",
        "        grad_logits = y_pred.copy()\n",
        "        grad_logits[range(batch_size), y_true] -= 1\n",
        "        grad_logits /= batch_size\n",
        "\n",
        "        # Gradients pour W et b\n",
        "        grad_W = np.dot(self.embedded_mean.T, grad_logits)\n",
        "        grad_b = np.sum(grad_logits, axis=0)\n",
        "\n",
        "        # Mise à jour\n",
        "        self.W -= learning_rate * grad_W\n",
        "        self.b -= learning_rate * grad_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Prédiction des classes\"\"\"\n",
        "        probs = self.forward(X)\n",
        "        return np.argmax(probs, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgPe_TOg13XV"
      },
      "source": [
        "**ENTRAÎNEMENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkG7ztaa135m",
        "outputId": "b1178452-f045-4207-d79d-3b5bb468aaa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== ENTRAÎNEMENT ===\n",
            "Epoch 1/10 - Loss: 0.6868 - Test Accuracy: 0.5608\n",
            "Epoch 2/10 - Loss: 0.6857 - Test Accuracy: 0.5608\n",
            "Epoch 3/10 - Loss: 0.6857 - Test Accuracy: 0.5608\n",
            "Epoch 4/10 - Loss: 0.6857 - Test Accuracy: 0.5608\n",
            "Epoch 5/10 - Loss: 0.6857 - Test Accuracy: 0.5608\n",
            "Epoch 6/10 - Loss: 0.6857 - Test Accuracy: 0.5608\n",
            "Epoch 7/10 - Loss: 0.6857 - Test Accuracy: 0.5608\n",
            "Epoch 8/10 - Loss: 0.6856 - Test Accuracy: 0.5608\n",
            "Epoch 9/10 - Loss: 0.6857 - Test Accuracy: 0.5608\n",
            "Epoch 10/10 - Loss: 0.6857 - Test Accuracy: 0.5608\n"
          ]
        }
      ],
      "source": [
        "# 11. ENTRAÎNEMENT\n",
        "TAILLE_VOCAB = len(vocabulaire)\n",
        "DIM_EMBEDDING = 50\n",
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "modele = ClassifieurSarcasme(TAILLE_VOCAB, DIM_EMBEDDING)\n",
        "\n",
        "print(\"\\n=== ENTRAÎNEMENT ===\")\n",
        "for epoch in range(EPOCHS):\n",
        "    # Mélanger les données\n",
        "    indices = np.random.permutation(len(X_train))\n",
        "    X_train_shuffled = X_train[indices]\n",
        "    y_train_shuffled = y_train[indices]\n",
        "\n",
        "    total_loss = 0\n",
        "    num_batches = len(X_train) // BATCH_SIZE\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        # Batch\n",
        "        start_idx = i * BATCH_SIZE\n",
        "        end_idx = start_idx + BATCH_SIZE\n",
        "        X_batch = X_train_shuffled[start_idx:end_idx]\n",
        "        y_batch = y_train_shuffled[start_idx:end_idx]\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = modele.forward(X_batch)\n",
        "\n",
        "        # Calcul de la loss\n",
        "        loss = cross_entropy_loss(y_pred, y_batch)\n",
        "        total_loss += loss\n",
        "\n",
        "        # Backward pass\n",
        "        modele.backward(X_batch, y_batch, y_pred, LEARNING_RATE)\n",
        "\n",
        "    # Évaluation sur le test\n",
        "    y_pred_test = modele.predict(X_test)\n",
        "    accuracy_test = np.mean(y_pred_test == y_test)\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f} - Test Accuracy: {accuracy_test:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdoJPJ3I2Bsl"
      },
      "source": [
        "**ÉVALUATION FINALE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14goQtoD2Im8",
        "outputId": "c845644b-5182-4f80-d199-0b026651981f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== ÉVALUATION FINALE ===\n",
            "Précision sur l'entraînement: 0.5611\n",
            "Précision sur le test: 0.5608\n"
          ]
        }
      ],
      "source": [
        "# 12. ÉVALUATION FINALE\n",
        "print(\"\\n=== ÉVALUATION FINALE ===\")\n",
        "y_pred_train = modele.predict(X_train)\n",
        "y_pred_test = modele.predict(X_test)\n",
        "\n",
        "accuracy_train = np.mean(y_pred_train == y_train)\n",
        "accuracy_test = np.mean(y_pred_test == y_test)\n",
        "\n",
        "print(f\"Précision sur l'entraînement: {accuracy_train:.4f}\")\n",
        "print(f\"Précision sur le test: {accuracy_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8TyNLlh2O2f"
      },
      "source": [
        "**TEST SUR DE NOUVEAUX EXEMPLES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuFRHfjx2PHH"
      },
      "outputs": [],
      "source": [
        "# 13. TEST SUR DE NOUVEAUX EXEMPLES\n",
        "def predire_sarcasme(texte, modele, vocab):\n",
        "    \"\"\"Prédit si un texte est sarcastique\"\"\"\n",
        "    # Prétraitement\n",
        "    texte_propre = pretraiter_texte(texte)\n",
        "    tokens = tokeniser(texte_propre)\n",
        "    indices = tokens_vers_indices(tokens, vocab, MAX_LENGTH)\n",
        "\n",
        "    # Prédiction\n",
        "    X_input = np.array([indices])\n",
        "    probs = modele.forward(X_input)\n",
        "    classe = np.argmax(probs[0])\n",
        "\n",
        "    return \"Sarcastique\" if classe == 1 else \"Non-sarcastique\", probs[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZn6dV2L2RC4"
      },
      "source": [
        "**Exemples de test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiahntZG2RS6",
        "outputId": "b48de46f-cd67-41b9-f0fe-0089a75c4cf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== PRÉDICTIONS SUR NOUVEAUX EXEMPLES ===\n",
            "\n",
            "Texte: Nation Suddenly Realizes This Just Going To Be A Thing That Happens From Now On\n",
            "Prédiction: Non-sarcastique\n",
            "Probabilités: Non-sarcastique=0.562, Sarcastique=0.438\n",
            "\n",
            "Texte: Scientists Discover New Species of Rare Butterfly in Amazon Rainforest\n",
            "Prédiction: Non-sarcastique\n",
            "Probabilités: Non-sarcastique=0.562, Sarcastique=0.438\n"
          ]
        }
      ],
      "source": [
        "# Exemples de test\n",
        "exemples = [\n",
        "    \"Nation Suddenly Realizes This Just Going To Be A Thing That Happens From Now On\",\n",
        "    \"Scientists Discover New Species of Rare Butterfly in Amazon Rainforest\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== PRÉDICTIONS SUR NOUVEAUX EXEMPLES ===\")\n",
        "for exemple in exemples:\n",
        "    prediction, probs = predire_sarcasme(exemple, modele, vocabulaire)\n",
        "    print(f\"\\nTexte: {exemple}\")\n",
        "    print(f\"Prédiction: {prediction}\")\n",
        "    print(f\"Probabilités: Non-sarcastique={probs[0]:.3f}, Sarcastique={probs[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
